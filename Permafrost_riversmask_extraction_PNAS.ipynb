{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-diagnosis",
   "metadata": {},
   "source": [
    "# River masks Semi-authomatic Multi-Temporal extraction from Landsat SR images usign Google Earth Engine\n",
    "### Variant of the original code that uses a service account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-pledge",
   "metadata": {},
   "source": [
    "### Niccolò Ragno, Riccardo Bonanomi, Marta Crivellaro, Alfonso Vitti, Guido Zolezzi and Marco Tubino\n",
    "* Publication corresponding Author: Niccolò Ragno,  niccolo.ragno@unitn.it\n",
    "* GEE code corresponding Author: Marta Crivellaro,  marta.crivellaro@unitn.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e238c30-9946-4565-a138-23b468ca74a5",
   "metadata": {},
   "source": [
    "GEE Python installation: https://developers.google.com/earth-engine/guides/python_install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-pasta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries import\n",
    "import os\n",
    "import pandas as pd\n",
    "from functions import areaImg, ndviMap, mndwiMap, histogram, otsu, peronaMalikFilter, check_tasks_status\n",
    "import ee\n",
    "import geemap\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1ecf5-41e6-43b5-875b-5919968b0438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Earth Engine authentication - see readme for details\n",
    "from credentials_script import get_credentials\n",
    "service_account, credentials_folder, credentials_file = get_credentials()\n",
    "credentials = ee.ServiceAccountCredentials(service_account, credentials_folder + credentials_file)\n",
    "ee.Initialize(credentials)\n",
    "\n",
    "# Google drive autentiucation - see readme for details\n",
    "gauth = GoogleAuth()\n",
    "scopes = ['https://www.googleapis.com/auth/drive']\n",
    "gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name(credentials_folder+credentials_file, scopes=scopes)\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549957e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import input data from function - see readme for details\n",
    "from input_data import get_input_data\n",
    "river_name, source, scaledLS, print_errors, delete_other, download_wait, coordinates, EPSG_code, dates, output_folder = get_input_data() # import the input data\n",
    "\n",
    "#1 - Define region(S) of interest (roi) as rectangular extents\n",
    "river_geom = ee.Geometry.Polygon(coordinates)\n",
    "\n",
    "# output data folders\n",
    "# local\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Google Drive\n",
    "drive_folder = river_name + '-' + 'Permafrost_rivermask_' + source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-thanksgiving",
   "metadata": {},
   "source": [
    "### Create an interactive map\n",
    "The default basemap is _Google Maps_. Additional basemaps can be added using the ``Map.add_basemap()`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map(center=[19.76,40.41], zoom=22)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d64790-4b5c-481c-9067-fa3027331103",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcGeom = river_geom\n",
    "roi = fcGeom \n",
    "Map.centerObject(roi)\n",
    "Map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-illustration",
   "metadata": {},
   "source": [
    "Standardise band names, merge Landsat data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "if source == 'ls':\n",
    "    # LANDSAT source\n",
    "    # Standardise band names, merge Landsat data:\n",
    "    bn8 = ['SR_B1', 'SR_B2', 'SR_B3', 'SR_B4', 'SR_B6', 'SR_QA_AEROSOL', 'SR_B5', 'SR_B7', 'QA_PIXEL']\n",
    "    bn7 = ['SR_B1', 'SR_B1', 'SR_B2', 'SR_B3', 'SR_B5', 'SR_CLOUD_QA'  , 'SR_B4', 'SR_B7', 'QA_PIXEL']\n",
    "    bn5 = ['SR_B1', 'SR_B1', 'SR_B2', 'SR_B3', 'SR_B5', 'SR_CLOUD_QA'  , 'SR_B4', 'SR_B7', 'QA_PIXEL']\n",
    "\n",
    "    #standard bands:\n",
    "    bnL = ['uBlue', 'Blue' , 'Green', 'Red'  , 'Swir1', 'BQA'          , 'Nir'  , 'Swir2', 'qa_pixel']\n",
    "\n",
    "    scale      = 30      # pixel size of Landsat images in meters\n",
    "    scale_hist = scale/2 # pixel size for the histogram calculation in meters\n",
    "    radio_res  = 65455\n",
    "\n",
    "    ## defining cloudmask function for landsat 7 and 8 only\n",
    "    # This function masks the input with a threshold on the simple cloud score.\n",
    "    # Observe that the input to simpleCloudScore() is a single Landsat TOA scene. \n",
    "    # Also note that simpleCloudScore() adds a band called ‘cloud’ to the input image. \n",
    "    # The cloud band contains the cloud score from 0 (not cloudy) to 100 (most cloudy).\n",
    "    def cloudMask(img):\n",
    "        cloudscore = ee.Algorithms.Landsat.simpleCloudScore(img).select('cloud')\n",
    "        return img.updateMask(cloudscore.lt(10))\n",
    "\n",
    "    def maskClouds(image):\n",
    "        \n",
    "        cloudShadowBitMask = (1 << 3)\n",
    "        cloudsBitMask = (1 << 5)\n",
    "        \n",
    "        qa = image.select('qa_pixel')\n",
    "        mask = (qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(qa.bitwiseAnd(cloudsBitMask).eq(0)))\n",
    "        \n",
    "        return image.updateMask(mask)\n",
    "\n",
    "    # calling LS Surface Reflectance image collections \n",
    "    ls5 = ee.ImageCollection(\"LANDSAT/LT05/C02/T1_L2\").filter(ee.Filter.lt('CLOUD_COVER',15)).select(bn5, bnL)\n",
    "    ls7 = (ee.ImageCollection(\"LANDSAT/LE07/C02/T1_L2\")  \\\n",
    "    .map(cloudMask)  \\\n",
    "    .filterDate('1999-04-15', '2003-05-30')  \\\n",
    "    .select(bn7, bnL))\n",
    "    ls8 = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\").filter(ee.Filter.lt('CLOUD_COVER', 15)).select(bn8, bnL)\n",
    "    # merging LS 5 and 8 dataset\n",
    "    ls = ls5.merge(ls8).sort('system:start', True).filter(ee.Filter.calendarRange(5,9,'month'))\n",
    "\n",
    "elif source == 's2':\n",
    "    # SENTINEL-2 data\n",
    "    # Standardise band names, merge Sentinel data:\n",
    "    bnS1 = ['B1', 'B2', 'B3', 'B4', 'B6', 'B5', 'B7','B8','B8A','B9','B11','B12','QA60']\n",
    "    # standard bands:\n",
    "    bns1 = ['Aerosols', 'Blue', 'Green', 'Red', 'RedEdge1', 'RedEdge2','RedEdge3','Nir','RedEdge4', 'WaterVapor','Swir1','Swir2','cldmask']\n",
    "\n",
    "    scale      = 10    # pixel size of Landsat images in meters\n",
    "    scale_hist = scale # pixel size for the histogram calculation in meters\n",
    "    radio_res  = 10000\n",
    "    \n",
    "    # calling Sentinel-2 MSI: MultiSpectral Instrument, Level-2A\n",
    "\n",
    "    def maskCloudsS2(image):\n",
    "        cloudsBitMask = (1 << 10)\n",
    "        cirrusBitMask = (1 << 11)\n",
    "        qa = image.select('cldmask')\n",
    "        mask = (qa.bitwiseAnd(cloudsBitMask).eq(0).And(qa.bitwiseAnd(cirrusBitMask).eq(0)))\n",
    "        return image.updateMask(mask)\n",
    "\n",
    "    s2 = ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\").select(bnS1, bns1)\\\n",
    "                 .sort('system:start', True).filterMetadata('CLOUDY_PIXEL_PERCENTAGE','less_than',15).map(maskCloudsS2).filter(ee.Filter.calendarRange(5,9,'month'))\n",
    "    \n",
    "else:\n",
    "    raise ValueError(\"data must be Landasat 'ls' or Sentinel-2 's2'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c92f4c-2058-4cec-9589-5d5dc8326428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions definition\n",
    "def RGBtoHSV (Image):\n",
    "    sat = Image.select(['Red','Green','Blue']).divide(radio_res).rgbToHsv().select(['saturation'])\n",
    "    return Image.addBands(sat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882a04f9-56d1-454b-8590-3add989d15cc",
   "metadata": {},
   "source": [
    "### Cycle on years to export annual domain mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512ca3d-3c0c-42f8-bea1-2a8081476b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_out = dates[:]\n",
    "ACy = {}\n",
    "data= []\n",
    "\n",
    "roi = fcGeom\n",
    "for year in dates:\n",
    "    sDate_T1 = str(year)+\"-05-01\"; \n",
    "    eDate_T1 = str(year)+\"-09-30\";\n",
    "    #Sort by:  roi, date:\n",
    "    if source == 'ls':\n",
    "        collection= ls \\\n",
    "            .filterBounds(roi) \\\n",
    "            .sort('system:start', True) \\\n",
    "            .filterDate(sDate_T1,eDate_T1)\n",
    "    elif source == 's2':\n",
    "        collection= s2 \\\n",
    "            .filterBounds(roi) \\\n",
    "            .sort('system:start', True) \\\n",
    "            .filterDate(sDate_T1,eDate_T1)\n",
    "\n",
    "    # Create a list of image objects.\n",
    "    imageList  = collection.toList(100);\n",
    "    Collection = collection.map(ndviMap).map(mndwiMap).map(RGBtoHSV)\n",
    "    median     = Collection.reduce(ee.Reducer.median())\n",
    "    maxi       = Collection.reduce(ee.Reducer.percentile([90]))\n",
    "    mini       = Collection.reduce(ee.Reducer.min())\n",
    "\n",
    "    ndvimap_median  = median.select('ndvi_median').clip(roi)\n",
    "    ndvimap_90p     = maxi.select('ndvi_p90').clip(roi)\n",
    "    mndwimap_median = median.select('mndwi_median').clip(roi)\n",
    "    satmap_90p      = maxi.select('saturation_p90').clip(roi)\n",
    "    ndwimap_90p     = maxi.select('mndwi_p90').clip(roi)\n",
    "\n",
    "    pm_mndwi_0_3 = peronaMalikFilter(mndwimap_median, 5, 2, 1, 0.3)\n",
    "    otsu_mndwi   = otsu(histogram(mndwimap_median, scale_hist).get('mndwi_median'  ))\n",
    "    otsu_sat     = otsu(histogram(satmap_90p     , scale_hist).get('saturation_p90'))\n",
    "    otsu_mndwiPM = otsu(histogram(pm_mndwi_0_3   , scale_hist).get('mndwi_median'  ))\n",
    "    \n",
    "    veg1 = ndvimap_90p.select('ndvi_p90').gte(0.15)\n",
    "    water3 =  satmap_90p.gt(otsu_sat).Or(ee.Image(mndwimap_median.select('mndwi_median')\n",
    "                                                  .gte(otsu_mndwi)).And(veg1.lt(1)))\n",
    "\n",
    "    waterPM3 = satmap_90p.gt(otsu_sat).Or(pm_mndwi_0_3.select('mndwi_median')\n",
    "                                          .gte(otsu_mndwiPM)).And(veg1.lt(1))\n",
    "\n",
    "    area_raw_N = areaImg(water3.  remap(ee.List([0]),ee.List([1])), scale)\n",
    "    area_pm_N  = areaImg(waterPM3.remap(ee.List([0]),ee.List([1])), scale)\n",
    "\n",
    "    if source == 's2': \n",
    "        if scaledLS:\n",
    "            pm_mndwi_0_3_scaledLS = peronaMalikFilter(mndwimap_median, 5, 6, 1, 0.3)\n",
    "            otsu_mndwiPM_scaledLS = otsu(histogram(pm_mndwi_0_3_scaledLS, scale_hist).get('mndwi_median'))\n",
    "            waterPM3_scaledLS     = satmap_90p.gt(otsu_sat).Or(pm_mndwi_0_3_scaledLS.select('mndwi_median').gte(otsu_mndwiPM_scaledLS)).And(veg1.lt(1))\n",
    "            area_pm_N_scaledLS = areaImg(waterPM3_scaledLS.remap(ee.List([0]),ee.List([1])), scale)\n",
    "        \n",
    "    try:\n",
    "        # extract the value as a number\n",
    "        area_raw_number = area_raw_N.getNumber('remapped').getInfo()\n",
    "        area_pm_number  = area_pm_N .getNumber('remapped').getInfo()\n",
    "        # dataframe with extracted areas and thresholds info creation and compiling\n",
    "        if source == 's2': \n",
    "            if scaledLS:\n",
    "                area_pm_number_scaledLS = area_pm_N_scaledLS.getNumber('remapped').getInfo()\n",
    "                data.append(dict(zip(('year','area_raw_number','area_pm_number','area_pm_number_scaledLS','%d','t_nmndwi','t_PMmndwi'),\n",
    "                            (str(year),area_raw_number,area_pm_number,area_pm_number_scaledLS,(100*((area_raw_number-area_pm_number)/area_raw_number)),otsu_mndwi.getInfo(),otsu_mndwiPM.getInfo(),))))\n",
    "            else:    \n",
    "                data.append(dict(zip(('year','area_raw_number','area_pm_number','%d','t_nmndwi','t_PMmndwi'),\n",
    "                        (str(year),area_raw_number,area_pm_number,(100*((area_raw_number-area_pm_number)/area_raw_number)),otsu_mndwi.getInfo(),otsu_mndwiPM.getInfo(),))))\n",
    "        else:\n",
    "            data.append(dict(zip(('year','area_raw_number','area_pm_number','%d','t_nmndwi','t_PMmndwi'),\n",
    "                        (str(year),area_raw_number,area_pm_number,(100*((area_raw_number-area_pm_number)/area_raw_number)),otsu_mndwi.getInfo(),otsu_mndwiPM.getInfo(),))))\n",
    "        \n",
    "        \n",
    "        #Export the images, specifying scale and region.\n",
    "        task = ee.batch.Export.image.toDrive(**{\n",
    "                'image': waterPM3.clip(fcGeom),\n",
    "                'description': river_name + '_' + str(year),\n",
    "                'folder': drive_folder,\n",
    "                'scale': scale,\n",
    "                'crs': EPSG_code,\n",
    "                'region': fcGeom\n",
    "\n",
    "            })\n",
    "        task.start()\n",
    "\n",
    "        if source == 's2': \n",
    "            if scaledLS:\n",
    "                task = ee.batch.Export.image.toDrive(**{\n",
    "                        'image': waterPM3_scaledLS.clip(fcGeom),\n",
    "                        'description': 'scaledLS_' + river_name + str(year),\n",
    "                        'folder': drive_folder,\n",
    "                        'scale': scale,\n",
    "                        'crs': EPSG_code,\n",
    "                        'region': fcGeom\n",
    "\n",
    "                    })\n",
    "                task.start()\n",
    "                    \n",
    "        print(str(year) + ' Done!')\n",
    "    except Exception as error:\n",
    "        if print_errors:\n",
    "            # handle the exception\n",
    "            print(\"An exception occurred:\", error) # An exception occurred: division by zero\n",
    "        print(str(year) + ' Error!')\n",
    "        dates_out.remove(year)\n",
    "\n",
    "print('Years where there are data:')\n",
    "print(dates_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ca477-3616-444c-9e88-843f677881c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the dataframe as csv file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(output_folder + river_name + '_' + source + '_stats.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc15d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_list = ee.data.listOperations()\n",
    "run, not_run = check_tasks_status(tasks_list)\n",
    "\n",
    "while run != 0 or not_run != 0:\n",
    "    print('There are still %d running tasks and %d task to start. Cheching again in a minute.' %(run, not_run))    \n",
    "    time.sleep(download_wait)\n",
    "    run, not_run = check_tasks_status()\n",
    "\n",
    "\n",
    "# download the files from Google Drive and delete them afterwards\n",
    "# list all folders in the root Drive folder\n",
    "folder_list = drive.ListFile({'q': \"'root' in parents and trashed=false\"}).GetList()\n",
    "\n",
    "if not folder_list:\n",
    "    print('No folders found in the root folder')\n",
    "else:\n",
    "    print('Downloading files from Google Drive')\n",
    "\n",
    "    # iterate over all folders\n",
    "    for folder in folder_list:\n",
    "        if not folder['title'] == drive_folder:\n",
    "            if delete_other:\n",
    "                print('Deleting folder: %s, id: %s' % (folder['title'], folder['id']))\n",
    "                folder.Delete()\n",
    "                continue\n",
    "            else:\n",
    "                print('Skipping folder: %s, id: %s' % (folder['title'], folder['id']))\n",
    "                continue\n",
    "        \n",
    "        saving_folder = output_folder + 'rivermask_' + source +  '/'\n",
    "        os.makedirs(saving_folder, exist_ok=True)\n",
    "        print('\\ntitle: %s, id: %s' % (folder['title'], folder['id']))\n",
    "\n",
    "        # list and download all files in the folder\n",
    "        file_list = drive.ListFile({'q': \"'%s' in parents and trashed=false\" % folder['id']}).GetList()\n",
    "\n",
    "        for file in file_list:\n",
    "            filename = file['title']\n",
    "            print('title: %s, id: %s' % (file['title'], file['id']))\n",
    "            \n",
    "            # download file into working directory (in this case a tiff-file)\n",
    "            file.GetContentFile(saving_folder + filename, mimetype=\"image/tiff\")\n",
    "\n",
    "            # delete file afterwards to keep the Drive empty\n",
    "            file.Delete()\n",
    "            \n",
    "        folder.Delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
